gpt-2-xl
====================
Number of Trainable Parameters (deduplicated)    : 2,127,057,600 params  |  8,508,230,400 bytes  (  7.924 GB)  |  69,699,423,436,800 FLOPs for seq_len=16384
Number of Trainable Parameters                   : 2,127,057,600 params  |  8,508,230,400 bytes  (  7.924 GB)  |  69,699,423,436,800 FLOPs for seq_len=16384
Number of Trainable Parameters (drop tok emb)    : 2,046,646,400 params  |  8,186,585,600 bytes  (  7.624 GB)  |  67,064,509,235,200 FLOPs for seq_len=16384
Number of Trainable Parameters (attn)            :   491,520,000 params  |  1,966,080,000 bytes  (  1.831 GB)  |  16,106,127,360,000 FLOPs for seq_len=16384
Number of Trainable Parameters (ffn)             : 1,554,971,200 params  |  6,219,884,800 bytes  (  5.793 GB)  |  50,953,296,281,600 FLOPs for seq_len=16384

============================================================
MATRIX MULTIPLICATION COUNTS
============================================================
Per Transformer Layer:
  Attention QKV projections   : 3
  Attention Q@K^T             : 1
  Attention attn@V            : 1
  Attention output proj       : 1
  FFN W1,W3 projections       : 2
  FFN W2 projection           : 1
  Total per layer             : 9

Total for 48 layers:
  Total attention operations  : 288
  Total FFN operations        : 144
  Total all layers            : 432

Final output:
  LM head projection          : 1

TOTAL MATRIX MULTIPLICATIONS  : 433